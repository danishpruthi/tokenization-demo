{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A large part of this demo (designed for pedagogical reasons) is borrowed from the awesome tutorial by Andrej Karpathy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "from tqdm import tqdm  \n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unicode Code points \n",
    "\n",
    "[Unicode](https://en.wikipedia.org/wiki/Unicode#Notes) defines 154998 characters from 168 scripts. Everything you can think of (including emojis) are likely defined, and can be represented using a unicode code point.\n",
    "\n",
    "This is a big deal as it enables digitalization of different writing systems. \n",
    "\n",
    "For instance, one of the most recent inclusions was the Tigalari script used to write Tulu (spoken not very far from Bangalore, in and around Mangalore).\n",
    "\n",
    "Let's look at some of the code points: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104\n",
      "2361\n"
     ]
    }
   ],
   "source": [
    "# code point for 'h'\n",
    "print (ord('h'))\n",
    "\n",
    "# code point for 'ह'\n",
    "print (ord('ह'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the Hindi 'ह' is assigned a higher number here. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hello_en = \"hello\"\n",
    "hello_hi = \"हैलो\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unicode code points is just an abstract concept where each (digitized) character is conceptually mapped to a number. However, to realize this concept in practice, `utf-8` is used to encode these code points. `utf-8` is designed to efficiently store and transmit data. \n",
    "\n",
    "It supports all valid code points, using a _variable-width_ encoding, ranging from 1 to 4 bytes. It is also backward-compatible with `ASCII`. (Side note: In standard ASCII-encoded data, there are unique values for just 128 alphabetic, numeric or special additional characters and control codes).\n",
    "\n",
    "Let's look at how some of the strings are encoded as per the `utf-8` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[104, 101, 108, 108, 111]\n",
      "[224, 164, 185, 224, 165, 136, 224, 164, 178, 224, 165, 139]\n"
     ]
    }
   ],
   "source": [
    "print (list(hello_en.encode('utf-8')))\n",
    "print (list(hello_hi.encode('utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ह [224, 164, 185]\n",
      "ै [224, 165, 136]\n",
      "ल [224, 164, 178]\n",
      "ो [224, 165, 139]\n"
     ]
    }
   ],
   "source": [
    "# let's look at the UTF-8 representation of individual hindi characters\n",
    "for ch in hello_hi:\n",
    "    print (ch, list(ch.encode('utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's train a tokenizer on Kohli's wikipedia page\n",
    "\n",
    "# english biography \n",
    "train_text_en = open('datasets/kohli_en.txt').read()\n",
    "\n",
    "# hindi biography\n",
    "train_text_hi = open('datasets/kohli_hi.txt').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English:  Virat Kohli (born 5 November 1988)[b] is an Indian international cricketer who plays Test and ODI cr\n",
      "Hindi:  विराट कोहली (जन्म: 5 नवम्बर 1988) भारतीय क्रिकेट टीम के एक दिवसीय क्रिकेट, टेस्ट क्रिकेट, व टी 20 आई\n"
     ]
    }
   ],
   "source": [
    "# printing parts of the biography\n",
    "\n",
    "print (\"English: \", train_text_en[:100])   \n",
    "print (\"Hindi: \", train_text_hi[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to compute the counts of byte pairs\n",
    "def get_counts(ids):\n",
    "    counts = {}\n",
    "\n",
    "    for i in range(1, len(ids)):\n",
    "        if (ids[i-1], ids[i]) not in counts:\n",
    "            counts[(ids[i-1], ids[i])] = 0\n",
    "        counts[(ids[i-1], ids[i])] += 1\n",
    "\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_en = list(train_text_en.encode('utf-8'))\n",
    "tokens_hi = list(train_text_hi.encode('utf-8'))\n",
    "\n",
    "stats_en = get_counts(tokens_en)\n",
    "stats_hi = get_counts(tokens_hi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e   1741\n",
      "  t 1573\n",
      "i n 1360\n",
      "t h 1298\n",
      "h e 1257\n",
      "s   1196\n",
      "  a 1139\n",
      "n   1025\n",
      "d   1009\n",
      "t   774\n",
      "a n 744\n",
      "e r 735\n",
      ",   697\n",
      "o r 684\n",
      "r e 677\n",
      "n g 666\n",
      "e d 652\n",
      "a t 639\n",
      "  s 627\n",
      "o n 598\n",
      "  o 560\n",
      "  i 535\n",
      "n d 530\n",
      "  h 514\n",
      "e n 514\n"
     ]
    }
   ],
   "source": [
    "# let's look at some of the most frequent byte pairs\n",
    "sorted_stats_en = sorted(stats_en.items(), key=lambda x: x[1], reverse=True)[:25]\n",
    "\n",
    "for (p0, p1), count in sorted_stats_en:\n",
    "    print (chr(p0), chr(p1), count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, some of the most popular byte-pairs correspond to characters you'd see frequently co-occur. For instance, `t` and `h`, `h` and `e`, `a` and `n`, `e` and `r`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to merge common pairs \n",
    "def merge(ids, pair, idx): \n",
    "    new_ids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "            # merge the pair -> idx\n",
    "            new_ids.append(idx)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_ids.append(ids[i])\n",
    "            i += 1\n",
    "\n",
    "    return new_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly test whether the merge function works? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert merge([1, 2, 3, 4, 5], (2, 3), 6) == [1, 6, 4, 5]\n",
    "assert merge([1, 2, 3, 4, 5], (9, 10), 2) == [1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 744/744 [00:08<00:00, 83.74it/s] \n"
     ]
    }
   ],
   "source": [
    "# let's train a tokenizer\n",
    "\n",
    "desired_vocab_size = 1000\n",
    "num_merges = desired_vocab_size - 256 \n",
    "\n",
    "# copy the tokens \n",
    "copy_tokens_en = tokens_en.copy()\n",
    "merges_en = {}\n",
    "\n",
    "for i in tqdm(range(num_merges)):\n",
    "\n",
    "    # compute counts \n",
    "    stats_en = get_counts(copy_tokens_en)\n",
    "\n",
    "    # find the most frequent pair\n",
    "    most_freq_pair = max(stats_en, key = lambda x: stats_en[x])\n",
    "\n",
    "    # merge the pair\n",
    "    copy_tokens_en = merge(copy_tokens_en, most_freq_pair, 256 + i)\n",
    "\n",
    "    # store the merges \n",
    "    merges_en[most_freq_pair] = 256 + i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the string \n",
    "\n",
    "def encode(text):\n",
    "    ids = list(text.encode('utf-8'))\n",
    "\n",
    "    for pair in merges_en:\n",
    "        ids = merge(ids, pair, merges_en[pair])\n",
    "\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the above encode function is not written in the most efficient way. For instance, one need not go over all the items in the merge-list and try to see which ones can be merged in the given text. (Doing the opposite would be more efficient.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's decode the ids\n",
    " \n",
    "# first 256 characters \n",
    "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "\n",
    "for (p0, p1), idx in merges_en.items():\n",
    "    vocab[idx] = vocab[p0] + vocab[p1]\n",
    "\n",
    "def decode(ids):\n",
    "    tokens = b\"\".join(vocab[idx] for idx in ids)\n",
    "    text = tokens.decode(\"utf-8\", errors='replace')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[285]\n"
     ]
    }
   ],
   "source": [
    "print (encode(\"Kohli\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's interesting to note that even with such a small number of merges `Kohli` gets a dedicated token--thanks to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[957, 355, 300, 341, 489, 104, 509, 114, 280]\n"
     ]
    }
   ],
   "source": [
    "print (encode(\"Virat Kohli scored a hundred\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Virat Kohli scored a hundred\n"
     ]
    }
   ],
   "source": [
    "print (decode(encode(\"Virat Kohli scored a hundred\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "科利得分一百\n"
     ]
    }
   ],
   "source": [
    "chinese_sample = \"科利得分一百\"\n",
    "print (decode(encode(chinese_sample)))\n",
    "assert (decode(encode(chinese_sample)) == chinese_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can verify how encoding and decoding work as inverse operations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
